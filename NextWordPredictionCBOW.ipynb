{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NextWordPredictionCBOW.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "u1NWMpekJZDM"
      ],
      "mount_file_id": "1nLTUsy5XWJNhZ-3HNtYeaaH-uLU7gRVi",
      "authorship_tag": "ABX9TyNqxlDVh7+mdwh8ul19lmP8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/oliverquintana/CBOWWordPrediction/blob/main/NextWordPredictionCBOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dbGTa7TRGXYx"
      },
      "source": [
        "# CBOW word predictor based on 1-word context\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p5Dl6ShDGklA"
      },
      "source": [
        "# Load dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kLqxBZdA1nzh"
      },
      "source": [
        "import json\n",
        "import nltk\n",
        "import spacy\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from utils import *\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PAMX1eVgt9jC"
      },
      "source": [
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1NWMpekJZDM"
      },
      "source": [
        "# Corpus Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXD17X3DLtyU"
      },
      "source": [
        "Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IJA5L5Hh17RY"
      },
      "source": [
        "corpus = readFile('corpus.txt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFofxeHAEgc5"
      },
      "source": [
        "Tokenize corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqT9GAW3VAtd"
      },
      "source": [
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens_str = tokenizer.tokenize(corpus)\n",
        "print('Tokens in corpus: {}'.format(len(tokens_str)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMBJJVEUEj_8"
      },
      "source": [
        "Clean corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XeechLxdnU4l"
      },
      "source": [
        "print('Input Corpus Size: {}'.format(len(tokens_str)))\n",
        "\n",
        "# Remove symbols \n",
        "punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
        "\n",
        "for i, token in enumerate(tokens_str):\n",
        "    n_token = ''\n",
        "    for char in token:\n",
        "        if char not in punc:\n",
        "            n_token += char\n",
        "\n",
        "    tokens_str[i] = n_token\n",
        "\n",
        "# Remove numeric tokens\n",
        "i = len(tokens_str) - 1\n",
        "while i >= 0:\n",
        "    if tokens_str[i] == '':\n",
        "        tokens_str.pop(i)\n",
        "    else:\n",
        "        for char in tokens_str[i]:\n",
        "            try:\n",
        "                int(char)\n",
        "                tokens_str.pop(i)\n",
        "            \n",
        "            except:\n",
        "                continue\n",
        "\n",
        "    i -= 1\n",
        "\n",
        "# Remove tokens with length < 2\n",
        "i = len(tokens_str) - 1\n",
        "while i >= 0:\n",
        "    if len(tokens_str[i]) < 2:\n",
        "        tokens_str.pop(i)\n",
        "\n",
        "    i -= 1\n",
        "\n",
        "# Remove stop words\n",
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "tokens_str = [token for token in tokens_str if token not in stop_words]\n",
        "\n",
        "print('Ouput Corpus Size: {}'.format(len(tokens_str)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVF8XtjyE6va"
      },
      "source": [
        "Lemmatization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KTGVDrPp27AR"
      },
      "source": [
        "nlp = spacy.load('es_core_news_sm')\n",
        "for i in range(len(tokens_str)):\n",
        "    tokens_str[i] = tokens_str[i].lower()\n",
        "    token = nlp(tokens_str[i])\n",
        "    lemmas = [tok.lemma_.lower() for tok in token]\n",
        "    tokens_str[i] = lemmas[0]\n",
        "    \n",
        "    if i % 1000 == 0:\n",
        "        print('Progress: {} / {}'.format(i, len(tokens_str)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eBj9yJw-F5qv"
      },
      "source": [
        "Preview corpus sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCokAar9T4Fh"
      },
      "source": [
        "for _ in range(10):\n",
        "    print(tokens_str[np.random.randint(len(tokens_str))])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QiD2YCl6Fhqn"
      },
      "source": [
        "Save clean corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h8p-0YaIT5Rf"
      },
      "source": [
        "str_out = ''\n",
        "file = open('corpus_clean.txt', 'w')\n",
        "for i, word in enumerate(tokens_str):\n",
        "    temp = word + ' '\n",
        "    file.write(temp)\n",
        "\n",
        "file.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LbaUu-lEIe1p"
      },
      "source": [
        "Save unique tokens with fixed indices"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQo6f1R7ILQV"
      },
      "source": [
        "dictTokens = vectDict(tokens)\n",
        "with open('dictTokens.txt', 'w') as outfile:\n",
        "    json.dump(dictTokens, outfile)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oznCRxowDX5D"
      },
      "source": [
        "# CBOW"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6hECcFzG47p"
      },
      "source": [
        "Define CBOW model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ka5Br0y0DbjM"
      },
      "source": [
        "class Cbow:\n",
        "    def __init__(self, vSize, cSize = 3, eSize = 100, lr = 0.001):\n",
        "\n",
        "        self.cSize = cSize\n",
        "        self.model = Sequential()\n",
        "        self.model.add(Dense(100, input_dim = vSize))\n",
        "        self.model.add(Dense(vSize, activation = 'softmax'))\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
        "        self.model.compile(loss = 'categorical_crossentropy', optimizer = opt)\n",
        "\n",
        "        self.model.summary()\n",
        "\n",
        "    def update_lr(lr = 0.0001):\n",
        "\n",
        "        opt = tf.keras.optimizers.Adam(learning_rate = lr)\n",
        "        self.model.compile(loss = 'categorical_crossentropy', optimizer = opt)\n",
        "\n",
        "    def train(self, corpus, dictTokens, epochs = 10, batch_size = 10, fname = 'cbow.h5'):\n",
        "\n",
        "        def getContextWords(corpus, dictTokens, batch_size):\n",
        "\n",
        "            indices = np.random.randint(self.cSize, len(corpus) - self.cSize, batch_size)\n",
        "            #X = np.zeros([batch_size, self.cSize*2, len(list(dictTokens.keys()))])\n",
        "            X = np.zeros([batch_size, self.cSize, len(list(dictTokens.keys()))])        # Context before wn only\n",
        "            y = np.zeros([batch_size, len(list(dictTokens.keys()))])\n",
        "\n",
        "            for i, index in enumerate(indices):\n",
        "                context = []\n",
        "                word = corpus[index]\n",
        "                context.extend(corpus[index-self.cSize : index])                        # Context before wn\n",
        "                #context.extend(corpus[index+1 : index+1+self.cSize])                   # Context after wn\n",
        "\n",
        "                y[i, dictTokens[word]] = 1\n",
        "                for j, context_word in enumerate(context):\n",
        "                    X[i, j, dictTokens[context_word]] = 1\n",
        "\n",
        "            return X, y\n",
        "\n",
        "        #steps = int(np.floor((len(corpus) - 2*self.cSize) / batch_size))\n",
        "        steps = int(np.floor((len(corpus) - self.cSize) / batch_size))\n",
        "        for epoch in range(epochs):\n",
        "            for step in range(steps):\n",
        "                X_batch, y_batch = getContextWords(corpus, dictTokens, batch_size)\n",
        "                X_batch = np.sum(X_batch, axis = 1)\n",
        "                loss = self.model.train_on_batch(X_batch, y_batch)\n",
        "                print('Epoch: {}/{} Step: {}/{} Loss: {}'.format(epoch, epochs, step, steps, loss))\n",
        "\n",
        "            self.model.save(fname)\n",
        "\n",
        "        return\n",
        "\n",
        "\n",
        "    def predict(self, indices, dictTokens, nPredictions = 3):\n",
        "\n",
        "        vocab = list(dictTokens.keys())\n",
        "        X = np.zeros([len(indices), len(vocab)], dtype = 'ushort')\n",
        "        for i, index in enumerate(indices):\n",
        "            X[i,index] = 1\n",
        "\n",
        "        pred = self.model.predict(X)\n",
        "        dPred = {}\n",
        "\n",
        "        for i in range(pred.shape[0]):\n",
        "            wPred = []\n",
        "            for _ in range(nPredictions):\n",
        "                index = np.argmax(pred[i])\n",
        "                word = vocab[index]\n",
        "                prob = pred[i,index]\n",
        "                wPred.append([word, prob])\n",
        "                pred[i,index] = 0\n",
        "\n",
        "            dPred[vocab[indices[i]]] = wPred\n",
        "            \n",
        "        for key, value in dPred.items():\n",
        "            s = ''\n",
        "            for x in value:\n",
        "                s += x[0] + '-' + str(np.round(x[1]*100, 3)) + '%' + ' '\n",
        "\n",
        "            print('Context: {} Predictions: {}'.format(key, s))\n",
        "\n",
        "        return"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mhb6MKthJQK-"
      },
      "source": [
        "Load corpus"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PoObvUxDeXr"
      },
      "source": [
        "corpus = readFile('corpus_clean.txt')\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "tokens = tokenizer.tokenize(corpus)\n",
        "stop_words = nltk.corpus.stopwords.words('spanish')\n",
        "tokens = [token for token in tokens if token not in stop_words]\n",
        "\n",
        "with open('dictTokens.txt') as json_file:\n",
        "    dictTokens = json.load(json_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ea-QrVwIJceF"
      },
      "source": [
        "Build CBOW model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0fieO2LgFavG",
        "outputId": "513ec5b5-0f86-421c-acf6-76a2991fb51d"
      },
      "source": [
        "cSize = 1                                           # Context size\n",
        "vSize = len(list(dictTokens.keys()))                # Vocabulary size for units in input and output layers\n",
        "lr = 0.001                                          # Learning rate\n",
        "model = Cbow(cSize = cSize, vSize = vSize, lr = lr) # Build model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense (Dense)                (None, 100)               4947800   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 49477)             4997177   \n",
            "=================================================================\n",
            "Total params: 9,944,977\n",
            "Trainable params: 9,944,977\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V7rEcGcrJ9Z8"
      },
      "source": [
        "Load pre-trained model weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L43-DDdcYCCz"
      },
      "source": [
        "model.model = tf.keras.models.load_model('cbow.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrnggQ9lKECG"
      },
      "source": [
        "Model training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qPNxOq00EG1w"
      },
      "source": [
        "model.train(tokens, dictTokens, epochs = 100, batch_size = 5000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dh536lSJKKog"
      },
      "source": [
        "Predict next word from context"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NwCM-aUAsU7y",
        "outputId": "35a62e12-6046-4ed3-8932-95da4ef057a9"
      },
      "source": [
        "samples = 5         # Number of examples to predict\n",
        "nPredictions = 3    # Predictions per sample\n",
        "indices = np.random.randint(0, len(list(dictTokens.keys())), samples)\n",
        "model.predict(indices, dictTokens = dictTokens, nPredictions = nPredictions)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Context: grotesco Predictions: joven-32.12% comer-20.331% solo-14.18% \n",
            "Context: cautivo Predictions: guapo-22.512% ahí-16.757% contar-9.144% \n",
            "Context: olor Predictions: particular-5.579% dermatólogo-3.626% fragante-3.007% \n",
            "Context: difuso Predictions: fusión-20.399% nebuloso-6.105% esconder-5.811% \n",
            "Context: corporal Predictions: hp-3.09% poder-2.977% solubilidad-2.511% \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usA_UswcKbes"
      },
      "source": [
        "Save trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNvK5wvkEiX5"
      },
      "source": [
        "model.model.save('cbow.h5')\n",
        "del model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vKaDJom9Ky2v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}